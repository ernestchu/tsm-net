<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
    <title>TSM-Net</title>
    <link rel="stylesheet" href="tufte.css"/>
    <link rel="stylesheet" href="latex.css"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
  </head>

  <body>
    <article>
      <h1 id="tufte-css">TSM-Net</h1>
      <h2>Temporal Compressing Autoencoder with Adversarial Losses for Time-Scale Modification on Audio Signals</h2>
      <p>Shao-Hsuan Chu, Ju-Ting Chu</p>
      <p>Department of Computer Science and Engineering, National Sun Yat-sen University</p>
      <section>
        <h2>Introduction</h2>
        <p>With the advance of technologies and digitalization, we can store and reproduce multimedia contents nowadays. We can even manipulate the materials in a way that we couldn't imagine before the digitalization. For example, image resizing and video editing, which changes the dimensionality of the digital pictures spatially and temporally, respectively. Another ubiquitous application regarding audio signals called time-scaled modification (TSM) is used in our daily life. It's also known as playback speed control in the video streaming platform such as YouTube.</p>
        <p>With the power of artificial intelligence (AI) and modern computation hardwares, however, we haven't discovered any approach using AI to refine TSM algorithm and leverage the quality of the synthetic audio to the next level. Consider we have pragmatic AI tools in similar domains like image super resolution and motion-compensated frame interpolation (MCFI).</p>
        <h3>Time-scale modification</h3>
        <p>The main idea of TSM is that instead of scaling the raw waveform on the time axis, which leads to pitch shifts due to the changes of wavelengths, we frame a sequence of samples, typically larger than the wavelength of the lowest-frequency, and relocate these frames in a overlapping fashion. However, the resultant sound is usually non-natural and contains audible clipping artifacts, this is the negative effect of the framing technique.</p>
        <p>Another approach tries to manipulate the audio in the spectral space, using short-time Fourier transform (STFT) to convert the frequency informations from the raw waveform to a more semantic representation, specifically magnitudes and phases. Unfortunately, unlike the magnitudes, which gives constructive and straightforward audio features, the phases is relatively complicated and hard to model. Moreover, due to the heavily correlation between each phase bins, we have to carefully relocate these bins to avoid artifacts, a.k.a. phasiness.</p>
        <h3>Harnessing the power of neural networks</h3>
      </section>
    </article>
  </body>
</html>
