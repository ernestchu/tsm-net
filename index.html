<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
    <title>TSM-Net</title>
    <link rel="stylesheet" href="tufte.css"/>
    <link rel="stylesheet" href="latex.css"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
  </head>

  <body>
    <article>
      <h1 id="tufte-css">TSM-Net</h1>
      <h2>Temporal Compressing Autoencoder with Adversarial Losses for Time-Scale Modification on Audio Signals</h2>
      <p>Shao-Hsuan Chu, Ju-Ting Chu</p>
      <p>Department of Computer Science and Engineering, National Sun Yat-sen University</p>
      <section>
        <h2>Abstract</h2>
        <p>We proposed a novel approach in the field of time-scale modification on the audio signals. Our neural network model encodes the raw audio into a latent representation called neuralgram. We apply some existing image resizing techniques on the neuralgram and decode it using our neural decoder to obtain the time-scaled audio. Our method yields little artifacts and opens a new possibility in the research of modern time-scale modification.</p>
      </section>
      <section>
        <h2>Introduction</h2>
        <p>With the advance of technologies and digitalization, we can store and reproduce multimedia contents nowadays. We can even manipulate the materials in a way that we couldn't imagine before the digitalization. For example, image resizing and video editing, which changes the dimensionality of the digital pictures spatially and temporally, respectively. Another ubiquitous application regarding audio signals called time-scaled modification (TSM) is used in our daily life. It's also known as playback speed control in the video streaming platform such as YouTube.</p>
        <p>With the power of artificial intelligence (AI) and modern computation hardwares, however, we haven't discovered any approach using AI to refine TSM algorithm and leverage the quality of the synthetic audio to the next level. Consider we have pragmatic AI tools in similar domains like image super resolution and motion-compensated frame interpolation (MCFI).</p>
        <h3>Time-scale modification</h3>
        <p>The main idea of TSM is that instead of scaling the raw waveforms on the time axis, which leads to pitch shifts due to the changes of wavelengths, we frame a sequence of samples, typically larger than the wavelength of the lowest-frequency, and relocate these frames in a overlapping fashion. However, the resultant sound is usually non-natural and contains audible clipping artifacts, this is the negative effect of the framing technique.</p>
        <p>Another approach tries to manipulate the audio in the spectral space, using short-time Fourier transform (STFT) to convert the frequency informations from the raw waveform to a more semantic representation, specifically magnitudes and phases. Unfortunately, unlike the magnitudes, which gives constructive and straightforward audio features, the phases is relatively complicated and hard to model. Moreover, due to the heavily correlation between each phase bins, we have to carefully relocate these bins to avoid artifacts, a.k.a. phasiness.</p>
        <h3>Harnessing the power of neural networks</h3>
        <p>As we've mentioned above, the task requires a highly temporally compressed representation of the audio signals. The neural networks naturally come into our minds. The neural networks are composed of a sequence of linear transformation joined by nonlinear activation functions. With numerous configurable parameters, also known as weights, they are capable of approximating almost any function we desire, including the compression function that transform the raw audio waveforms into low-dimensional latent vectors and back.</p>
        <p>The parameters are initially random noises and can be gradually inferred using a technique called gradient descent, in which we specify a meaningful loss function such as the difference between the networks' output and the target value, then the parameters can be configured based on the gradients of the loss function so the output would slowly move toward the target value.</p>
        <p>To transform back and forth in two domains, we can think of the neural networks as an encoder and a decoder, in the jargon of machine learning, this kind of architecture is called autoencoder, which encodes the data into a high-level (and typically low-dimensional) latent vectors and tries its best to decode it back to the original data. In our neural network model, the dimension of the latent vectors is 1024 times smaller than the original one, which means one sample in the latent vector can represent more than an entire wave. Since the latent vector is an image-like multi-dimension vector, we can apply the existing image resizing techniques to scale it. Finally, we decode the resized latent vector to obtain the time-scaled audio waveform.</p>
      </section>
    </article>
  </body>
</html>
